{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a7277a",
   "metadata": {},
   "source": [
    "# Causal effect estimation with external controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7711d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "\n",
    "import warnings\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "968c0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../scripts/auxilary.py\"\n",
    "%run \"../scripts/dml_ate_fusion.py\"\n",
    "%run \"../scripts/simu.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887bfcd8",
   "metadata": {},
   "source": [
    "We consider a simulation setting where we have a relatively small (n = 200) target experimental data, with two large (n = 1000) source data. This is a common setting in practice. Source data are heterogeneous with both covariate shift and label shift. However, the conditional outcome on the control is the same as the target data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac2199c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration for the simulation\n",
    "# nT: sample size of target data\n",
    "# nS_1: sample size of source data 1\n",
    "# nS_2: sample size of source data 2\n",
    "# n_reps: number of simulation repetitions\n",
    "# seed: random seed for reproducibility\n",
    "CONFIG = {\n",
    "    \"nT\": 200,\n",
    "    \"nS_1\": 1000,\n",
    "    \"nS_2\": 1000,\n",
    "    \"n_reps\": 1000, \n",
    "    \"seed\": 20250901\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ce2949",
   "metadata": {},
   "source": [
    "Our estimator requires three ML models: a mean model, a propensity score model, and a variance model. Here we consider random forest as an example, but other advanced ML models such as XGBoost, LightGBM, and neural networks, are also readily incorporated. \n",
    "\n",
    "In addition to these ML models, we also require an estimator of the density ratio. Here we use kernel density estimation, but other methods such as logistic regression or entropy balcancing, are also available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b16923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to be used for nuisance function estimation, here we use random forest as an example\n",
    "mean_model = RandomForestRegressor(n_estimators=100)\n",
    "p_score_model = RandomForestRegressor(n_estimators=100)\n",
    "var_model = RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "204b48ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchangeable functional\n",
    "mu_0 = lambda U1, U2: 1 + 0.5*U1 + U2\n",
    "\n",
    "# Functions to generate the target data (The true average treatment effect is 2)\n",
    "target_p_score = lambda U1, U2: 1/(1+np.exp(-0.2*U1-0.3*U2-np.sign(U2)*1.5*U1**2))\n",
    "target_mu_1 = lambda U1, U2: 3 + 0.5*U1 + 1.5*U2\n",
    "target_sigma_0 = lambda U1, U2: 0.2 * np.abs(U1)**0.4\n",
    "target_sigma_1 = lambda U1, U2: 0.2 * np.abs(U2)**0.2\n",
    "\n",
    "# Functions to generate the 1st source data (The average treatment effect is 0)\n",
    "source_1_p_score = lambda U1, U2: 1/(1+np.exp(-0.2*U1-0.3*U2))\n",
    "source_1_mu_1 = lambda U1, U2:  1+ 0.5*U2 + 1.5*U1\n",
    "source_1_sigma_0 = lambda U1, U2: 0.2 * np.abs(U1)**0.2\n",
    "source_1_sigma_1 = lambda U1, U2: 0.2 * np.abs(U2)**0.2\n",
    "\n",
    "# Functions to generate the 2nd source data (The average treatment effect is -1)\n",
    "source_2_p_score = lambda U1, U2: 1/(1+np.exp(-0.2*U1-0.3*U2))\n",
    "source_2_mu_1 = lambda U1, U2:  -U1 + U2\n",
    "source_2_sigma_0 = lambda U1, U2: 0.2 * np.abs(U1)**0.4\n",
    "source_2_sigma_1 = lambda U1, U2: 0.2 * np.abs(U2)**0.4\n",
    "\n",
    "# Except the exchangeable functional, all other functions are heterogeneous between target and source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d297373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MainSimu(nT, nS, seed, n_reps,\n",
    "             mu_0, target_p_score, target_mu_1, target_sigma_0, target_sigma_1, \n",
    "             source_p_score, source_mu_1, source_sigma_0, source_sigma_1, \n",
    "             mean_mode, p_score_model, var_model):\n",
    "    \"\"\"\n",
    "    Main simulation function to run DML_ATE multiple times and summarize the results. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int, number of samples in each simulation\n",
    "    seed : int, random seed\n",
    "    n_reps : int, number of repetitions\n",
    "    mu_0 : function of U1, U2, mean function of Y when A=0 on both target and source data\n",
    "    target_p_score : function of U1, U2, propensity score function on target data\n",
    "    target_mu_1 : function of U1, U2, mean function of Y when A=1 on target data\n",
    "    target_sigma_0 : function of U1, U2, variance function of Y when A=0 on target data\n",
    "    target_sigma_1 : function of U1, U2, variance deviation function of Y when A=1 on target data\n",
    "    source_p_score : list of functions of U1, U2, propensity score function on each source data\n",
    "    source_mu_1 : list of functions of U1, U2, mean function of Y when A=1 on each source data\n",
    "    source_sigma_0 : list of functions of U1, U2, variance function of Y when A=0 on each source data\n",
    "    source_sigma_1 : list of functions of U1, U2, variance deviation function of Y when A=1 on each source data\n",
    "    mean_model: machine learning model class, estimate the conditional mean given covariates\n",
    "    p_score_model: machine learning model class, estimate the propensity score given covariates\n",
    "    var_model: machine learning model class, estimate the conditional variance given covariates\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with columns \"Coverage\", \"Bias\", \"Std_Error\", DML ATE with target data only\n",
    "    pd.DataFrame with columns \"Coverage\", \"Bias\", \"Std_Error\", DML ATE with all data\n",
    "\n",
    "    \"\"\"\n",
    "    # True ATE\n",
    "    true_ATE = 2.0\n",
    "    coverages_fusion = []\n",
    "    biases_fusion = []\n",
    "    std_errors_fusion = []\n",
    "    coverages = []\n",
    "    biases = []\n",
    "    std_errors = []\n",
    "    for rep in trange(n_reps, desc=\"Simulations\"):\n",
    "        rep_seed = seed + rep\n",
    "        np.random.seed(rep_seed)\n",
    "        \n",
    "        # Generate target data and estimate ATE using target data only\n",
    "        target_data = SimuData(nT, target_p_score, mu_0, target_mu_1, target_sigma_0, target_sigma_1)\n",
    "        ATE_est, ATE_std = DML_ATE(target_data, p_score_model, mean_model)\n",
    "\n",
    "        # Generate source data and estimate ATE using all data\n",
    "        source_data = [SimuData(nS, source_p_score[m], mu_0, source_mu_1[m], source_sigma_0[m], source_sigma_1[m]) for m in range(len(source_p_score))]\n",
    "        ATE_est_fusion, ATE_std_fusion = DML_ATE_fusion(target_data, source_data, p_score_model, mean_model, var_model, density_ratio)\n",
    "        coverage_fusion = (ATE_est_fusion - 1.96 * ATE_std_fusion <= true_ATE) and (ATE_est_fusion + 1.96 * ATE_std_fusion >= true_ATE)\n",
    "        bias_fusion = np.abs(ATE_est_fusion - true_ATE)\n",
    "        std_error_fusion = ATE_std_fusion\n",
    "        coverages_fusion.append(coverage_fusion)\n",
    "        biases_fusion.append(bias_fusion)\n",
    "        std_errors_fusion.append(std_error_fusion)\n",
    "\n",
    "        coverage = (ATE_est - 1.96 * ATE_std <= true_ATE) and (ATE_est + 1.96 * ATE_std >= true_ATE)\n",
    "        bias = np.abs(ATE_est - true_ATE)\n",
    "        std_error = ATE_std\n",
    "        coverages.append(coverage)\n",
    "        biases.append(bias)\n",
    "        std_errors.append(std_error)\n",
    "    result = pd.DataFrame({\"Coverage\": coverages, \"Bias\": biases, \"Std_Error\": std_errors})\n",
    "    result_fusion = pd.DataFrame({\"Coverage\": coverages_fusion, \"Bias\": biases_fusion, \"Std_Error\": std_errors_fusion}) \n",
    "    return result, result_fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed702b9e",
   "metadata": {},
   "source": [
    "Simulation is repeated for 1,000 times, and the coverage and standard error of two estimators, the classical DML estimator for ATE and our data fusion estimator, are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b82e215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulations: 100%|██████████| 1000/1000 [1:31:51<00:00,  5.51s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    result, result_fusion = MainSimu(CONFIG[\"nT\"], CONFIG[\"nS_1\"], CONFIG[\"seed\"], CONFIG[\"n_reps\"],\n",
    "                                     mu_0, target_p_score, target_mu_1, target_sigma_0, target_sigma_1, \n",
    "                                     [source_1_p_score, source_2_p_score], [source_1_mu_1, source_2_mu_1], \n",
    "                                     [source_1_sigma_0, source_2_sigma_0], [source_1_sigma_1, source_2_sigma_1], \n",
    "                                     mean_model, p_score_model, var_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e058607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.905 0.817855910143713 0.67931832731549\n",
      "0.948 0.5972526514731746 0.5865094149401825\n"
     ]
    }
   ],
   "source": [
    "print(result[\"Coverage\"].mean(), result[\"Bias\"].mean(), result[\"Std_Error\"].mean())\n",
    "print(result_fusion[\"Coverage\"].mean(), result_fusion[\"Bias\"].mean(), result_fusion[\"Std_Error\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636824aa",
   "metadata": {},
   "source": [
    "Due to a small sample target data (n = 200), coverage of the classical DML estimator is slightly below 95% (coverage = 0.905). In contrast, our data fusion estimator has valid coverage (0.948), and leads to 13.7% reduction in standard error (or 34% increase in effective sample size). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f0ee6",
   "metadata": {},
   "source": [
    "We note that augmentation is asymptotically equivalent to pooling all data to estimate the conditional mean on the control arm. Therefore, the following three versions of the data fusion algorithms are all equivalent: (i) estimate the conditional mean on the control with all data, no augmentation term (ii) estimate the conditional mean on the control with only the target data, with the augmentation term (iii) estimate the conditional mean on the control with all data, with the augmentation term. For finite-sample performance, (iii) is used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
